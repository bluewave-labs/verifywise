"""Main evaluation runner with checkpoint/resume support."""

import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

from ..providers import get_provider, BaseProvider
from ..judge import GPT4Judge


@dataclass
class ModelResult:
    """Results for a single model evaluation."""
    model_id: str
    model_name: str
    provider: str
    benchmark_scores: Dict[str, List[float]]
    usecase_scores: Dict[str, List[float]]
    aggregated: Dict[str, float]
    errors: List[str]
    completed_at: str


class EvaluationRunner:
    """Orchestrates evaluation of multiple models with checkpoint/resume."""
    
    def __init__(
        self,
        models_config: Dict,
        eval_config: Dict,
        judge: GPT4Judge,
        output_dir: Path,
    ):
        self.models_config = models_config
        self.eval_config = eval_config
        self.judge = judge
        self.output_dir = output_dir
        self.checkpoint_path = output_dir / "checkpoint.json"
        self.results: List[ModelResult] = []
        self.completed_models: set = set()
        
        # Load scenarios
        self.scenarios = self._load_scenarios()
        self.usecase_samples = self._load_usecase_samples()
    
    def _load_scenarios(self) -> List[Dict]:
        """Load benchmark scenarios from llm-leaderboard dataset."""
        scenarios = []
        benchmark_config = self.eval_config.get("benchmark", {})
        data_path = Path(benchmark_config.get("data_path", "../llm-leaderboard/datasets/v0.1"))
        scenarios_file = data_path / "scenarios.jsonl"
        
        if scenarios_file.exists():
            with open(scenarios_file) as f:
                for line in f:
                    if line.strip():
                        scenario = json.loads(line)
                        scenarios.append(scenario)
        else:
            print(f"Warning: Scenarios file not found: {scenarios_file}")
        
        # Limit scenarios if configured
        max_per_type = benchmark_config.get("max_scenarios_per_type", 30)
        if max_per_type:
            by_type: Dict[str, List] = {}
            for s in scenarios:
                stype = s.get("scenario_type", "unknown")
                if stype not in by_type:
                    by_type[stype] = []
                if len(by_type[stype]) < max_per_type:
                    by_type[stype].append(s)
            scenarios = [s for lst in by_type.values() for s in lst]
        
        return scenarios
    
    def _load_usecase_samples(self) -> List[Dict]:
        """Load use-case evaluation samples."""
        samples = []
        usecase_config = self.eval_config.get("usecase", {})
        data_path = Path(usecase_config.get("data_path", "data/usecase_samples.jsonl"))
        
        if data_path.exists():
            with open(data_path) as f:
                for line in f:
                    if line.strip():
                        samples.append(json.loads(line))
        else:
            # Generate default samples if file doesn't exist
            samples = self._generate_default_usecase_samples()
        
        return samples
    
    def _generate_default_usecase_samples(self) -> List[Dict]:
        """Generate default use-case samples for evaluation."""
        return [
            {
                "id": "uc_001",
                "prompt": "Explain how photosynthesis works in plants.",
                "reference": "Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen. It occurs in the chloroplasts, where chlorophyll absorbs light energy.",
            },
            {
                "id": "uc_002", 
                "prompt": "What are the main causes of climate change?",
                "reference": "The main causes of climate change include burning fossil fuels, deforestation, industrial processes, and agriculture, all of which increase greenhouse gas concentrations in the atmosphere.",
            },
            {
                "id": "uc_003",
                "prompt": "Describe the process of machine learning model training.",
                "reference": "Machine learning model training involves feeding data to an algorithm, which learns patterns through iterative optimization of parameters to minimize prediction errors.",
            },
            {
                "id": "uc_004",
                "prompt": "What is the capital of France and what is it known for?",
                "reference": "Paris is the capital of France. It is known for the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and its rich history in art, fashion, and cuisine.",
            },
            {
                "id": "uc_005",
                "prompt": "Explain the difference between HTTP and HTTPS.",
                "reference": "HTTP is the standard protocol for web communication, while HTTPS adds SSL/TLS encryption for secure data transmission, protecting against eavesdropping and tampering.",
            },
        ]
    
    def load_checkpoint(self) -> bool:
        """Load checkpoint if exists. Returns True if checkpoint found."""
        if self.checkpoint_path.exists():
            with open(self.checkpoint_path) as f:
                checkpoint = json.load(f)
            
            self.completed_models = set(checkpoint.get("completed_models", []))
            self.results = [
                ModelResult(**r) for r in checkpoint.get("results", [])
            ]
            print(f"Loaded checkpoint: {len(self.completed_models)} models completed")
            return True
        return False
    
    def save_checkpoint(self):
        """Save current progress to checkpoint file."""
        checkpoint = {
            "completed_models": list(self.completed_models),
            "results": [asdict(r) for r in self.results],
            "saved_at": datetime.now().isoformat(),
        }
        with open(self.checkpoint_path, "w") as f:
            json.dump(checkpoint, f, indent=2)
    
    def get_all_models(self) -> List[tuple]:
        """Get list of (model_config, provider_name) tuples."""
        models = []
        providers = self.models_config.get("providers", {})
        
        for provider_name, provider_config in providers.items():
            if provider_name == "judge":
                continue
            for model in provider_config.get("models", []):
                models.append((model, provider_name, provider_config))
        
        return models
    
    async def evaluate_single_model(
        self,
        model_config: Dict,
        provider_name: str,
        provider_config: Dict,
    ) -> ModelResult:
        """Evaluate a single model against all scenarios."""
        
        model_id = model_config["id"]
        model_name = model_config["name"]
        model_provider = model_config["provider"]
        
        # Get provider instance
        provider = get_provider(provider_name, provider_config)
        
        # Initialize result containers
        benchmark_scores = {
            "compliance_policy": [],
            "ambiguous_prompt": [],
            "multi_step_reasoning": [],
        }
        usecase_scores = {
            "correctness": [],
            "completeness": [],
            "relevancy": [],
            "bias": [],
            "toxicity": [],
            "hallucination": [],
        }
        errors = []
        
        # === BENCHMARK EVALUATION ===
        for scenario in self.scenarios:
            try:
                # Get model response
                response = await provider.complete_with_retry(
                    model=model_id,
                    prompt=scenario["prompt"],
                    max_tokens=500,
                )
                
                # Judge with GPT-4
                judgment = await self.judge.grade_benchmark(scenario, response)
                
                # Convert grade to score
                grade_scores = {"good": 100, "risky": 50, "bad": 0}
                score = grade_scores.get(judgment.get("grade", "bad"), 0)
                
                scenario_type = scenario.get("scenario_type", "unknown")
                if scenario_type in benchmark_scores:
                    benchmark_scores[scenario_type].append(score)
                
            except Exception as e:
                errors.append(f"Benchmark {scenario.get('scenario_id', 'unknown')}: {str(e)}")
        
        # === USE-CASE EVALUATION ===
        for sample in self.usecase_samples:
            try:
                response = await provider.complete_with_retry(
                    model=model_id,
                    prompt=sample["prompt"],
                    max_tokens=500,
                )
                
                # Evaluate each metric
                for metric in usecase_scores.keys():
                    judgment = await self.judge.grade_usecase(
                        metric=metric,
                        prompt=sample["prompt"],
                        response=response,
                        reference=sample.get("reference"),
                    )
                    usecase_scores[metric].append(judgment.get("score", 0))
                
            except Exception as e:
                errors.append(f"UseCase {sample.get('id', 'unknown')}: {str(e)}")
        
        # === AGGREGATE SCORES ===
        aggregated = self._aggregate_scores(benchmark_scores, usecase_scores)
        
        return ModelResult(
            model_id=model_id,
            model_name=model_name,
            provider=model_provider,
            benchmark_scores=benchmark_scores,
            usecase_scores=usecase_scores,
            aggregated=aggregated,
            errors=errors,
            completed_at=datetime.now().isoformat(),
        )
    
    def _aggregate_scores(
        self,
        benchmark_scores: Dict[str, List[float]],
        usecase_scores: Dict[str, List[float]],
    ) -> Dict[str, float]:
        """Compute aggregated scores with weights."""
        
        def avg(lst):
            return sum(lst) / len(lst) if lst else 0
        
        # Map scenario types to score names
        aggregated = {
            "compliance": round(avg(benchmark_scores.get("compliance_policy", [])), 1),
            "ambiguity": round(avg(benchmark_scores.get("ambiguous_prompt", [])), 1),
            "reasoning": round(avg(benchmark_scores.get("multi_step_reasoning", [])), 1),
            "correctness": round(avg(usecase_scores.get("correctness", [])), 1),
            "completeness": round(avg(usecase_scores.get("completeness", [])), 1),
            "relevancy": round(avg(usecase_scores.get("relevancy", [])), 1),
            # Invert bias/toxicity/hallucination (lower is better in raw, higher is better in display)
            "bias": round(100 - avg(usecase_scores.get("bias", [])), 1),
            "toxicity": round(100 - avg(usecase_scores.get("toxicity", [])), 1),
            "hallucination": round(100 - avg(usecase_scores.get("hallucination", [])), 1),
        }
        
        # Calculate overall score with weights
        weights = self.eval_config.get("weights", {})
        default_weights = {
            "compliance": 0.15, "ambiguity": 0.10, "reasoning": 0.15,
            "correctness": 0.15, "completeness": 0.10, "relevancy": 0.10,
            "bias": 0.10, "toxicity": 0.05, "hallucination": 0.10,
        }
        weights = {**default_weights, **weights}
        
        overall = sum(aggregated.get(k, 0) * w for k, w in weights.items())
        aggregated["overall"] = round(overall, 1)
        
        return aggregated
    
    async def run(self, resume: bool = True):
        """Run full evaluation with optional resume."""
        
        # Load checkpoint if resuming
        if resume:
            self.load_checkpoint()
        
        # Get models to evaluate
        all_models = self.get_all_models()
        remaining = [
            (m, pn, pc) for m, pn, pc in all_models 
            if m["id"] not in self.completed_models
        ]
        
        print(f"\nTotal models: {len(all_models)}")
        print(f"Already completed: {len(self.completed_models)}")
        print(f"Remaining: {len(remaining)}")
        print(f"Scenarios per model: {len(self.scenarios)} benchmark + {len(self.usecase_samples)} use-case")
        
        if not remaining:
            print("\nAll models already evaluated!")
            return self.results
        
        # Evaluate remaining models
        checkpoint_interval = self.eval_config.get("runtime", {}).get("checkpoint_interval", 10)
        
        for i, (model_config, provider_name, provider_config) in enumerate(remaining):
            model_id = model_config["id"]
            model_name = model_config["name"]
            
            print(f"\n[{i+1}/{len(remaining)}] Evaluating: {model_name} ({provider_name})")
            
            try:
                result = await self.evaluate_single_model(
                    model_config, provider_name, provider_config
                )
                self.results.append(result)
                self.completed_models.add(model_id)
                
                print(f"  ‚úì Overall: {result.aggregated.get('overall', 0)}%")
                if result.errors:
                    print(f"  ‚ö† {len(result.errors)} errors")
                
            except Exception as e:
                print(f"  ‚úó Failed: {e}")
                self.results.append(ModelResult(
                    model_id=model_id,
                    model_name=model_name,
                    provider=model_config["provider"],
                    benchmark_scores={},
                    usecase_scores={},
                    aggregated={},
                    errors=[str(e)],
                    completed_at=datetime.now().isoformat(),
                ))
                self.completed_models.add(model_id)
            
            # Save checkpoint periodically
            if (i + 1) % checkpoint_interval == 0:
                self.save_checkpoint()
                print(f"  üìÅ Checkpoint saved ({len(self.completed_models)} models)")
        
        # Final checkpoint
        self.save_checkpoint()
        print(f"\n‚úÖ Evaluation complete! {len(self.results)} models evaluated.")
        
        return self.results
