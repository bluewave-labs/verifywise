,"IBM AI Risk Database
A structured library of AI risks curated by IBM, covering agentic behaviors, data leakage, bias, security issues, and operational failures. Each risk entry includes a clear description, categories, likelihood, and severity so teams can assess, compare, and prioritize risks when deploying advanced AI systems.",,,,
,Agentic AI risks,,,,
,,,,,
,Risk name,Description,Categories,Likelihood,Severity
,Unexplainable and untraceable actions,"The agent may take actions that can’t be traced back to a clear reasoning path. This makes troubleshooting, auditing and incident investigation difficult. It also reduces trust in the system.","Operational risk, Technological risk, Compliance risk",Possible,Moderate
,Sharing IP or confidential information with the user,The agent might surface proprietary data or personal information in natural-language outputs. Leaks often go unnoticed and create legal and reputational exposure. Even small slips can be irreversible.,"Data privacy risk, Legal risk, Reputational risk, Cybersecurity risk",Possible,Major
,Sharing IP or confidential information with tools,"External tools or APIs may receive sensitive data unintentionally when the agent passes context outward. Once data enters third-party systems, control and visibility are lost.","Third-party or vendor risk, Cybersecurity risk, Data privacy risk, Legal risk",Likely,Major
,Discriminatory actions,"The agent may treat groups unfairly due to biased reasoning or skewed training patterns. This affects service quality, hiring decisions or customer treatment. It can cause serious compliance and reputation damage.","Compliance risk, Legal risk, Reputational risk, Human resources risk",Possible,Major
,Introduction of new data bias,Agents can generate new biased content or reinforce skewed data patterns. These issues spread across downstream systems and create long-term fairness gaps.,"Operational risk, Technological risk, Compliance risk",Possible,Moderate
,Impact on human dignity,Agents may interact in ways that feel disrespectful or dehumanizing. This undermines user trust and can conflict with internal ethical standards. In sensitive contexts it causes real harm.,"Reputational risk, Compliance risk, Human resources risk",Unlikely,Moderate
,Impact on human agency,Highly autonomous agents can take over decisions that should remain under human control. This reduces oversight and can create overdependence on automated judgment.,"Strategic risk, Operational risk, Compliance risk",Possible,Major
,Impact on jobs,Agentic automation reshapes job roles and can displace certain tasks entirely. Teams may face uncertainty and morale issues as responsibilities shift.,"Human resources risk, Strategic risk",Likely,Moderate
,Environmental impact,"Agents may loop or execute unnecessary actions, increasing compute consumption. Over time this leads to higher energy use and larger cloud costs.","Environmental risk, Financial risk",Possible,Minor
,Over-reliance or under-reliance on agents,"Some users trust the agent too much, others ignore it entirely. Both extremes undermine safety and effectiveness and introduce avoidable errors.","Operational risk, Strategic risk",Possible,Moderate
,Misaligned actions,The agent may pursue goals that technically match the prompt but conflict with actual intentions. These errors lead to harmful or wasteful outcomes.,"Operational risk, Technological risk, Strategic risk",Possible,Major
,Attacks on external resources used by the agent,"Agents depend on external APIs, tools and services. If attackers compromise these components, the agent inherits the risk and may behave unpredictably.","Cybersecurity risk, Third-party or vendor risk, Operational risk",Possible,Major
,Unauthorized use,"Without strong access controls, an agent can be triggered by someone who shouldn't have access. This results in unintended actions, data exposure or policy violations.","Cybersecurity risk, Operational risk, Compliance risk",Possible,Major
,Exploiting trust mismatch,Users often assume the agent is more reliable or capable than it actually is. This misplaced trust can be exploited or lead to poor decisions.,"Reputational risk, Operational risk, Human resources risk",Possible,Moderate
,Function-calling hallucination,The agent may trigger tool calls based on incorrect assumptions or invented context. These hallucinated actions can modify systems or data in dangerous ways.,"Technological risk, Operational risk, Cybersecurity risk",Possible,Major
,Redundant actions,Agents may repeat tasks or run unnecessary steps because of misinterpreted state or looping behavior. This wastes compute resources and slows workflows.,"Operational risk, Environmental risk, Financial risk",Likely,Minor
,Incomplete agent evaluation,Many agent capabilities aren’t covered in standard testing. This creates blind spots where failures emerge only in real-world conditions.,"Compliance risk, Operational risk, Technological risk",Possible,Major
,Mitigation and maintenance challenges,"Agent behavior changes as tools, environments or models evolve. Maintaining safe behavior requires constant monitoring and policy updates over time.","Operational risk, Strategic risk, Compliance risk",Likely,Moderate
,Lack of agent transparency,The agent’s internal reasoning and state transitions are hard to inspect. This makes it difficult to audit decisions and understand harmful outcomes.,"Compliance risk, Operational risk, Reputational risk",Possible,Major
,Reproducibility issues,"Agent behavior can differ between runs due to randomness, tool timing or environmental differences. This complicates debugging, evaluation and incident analysis.","Technological risk, Operational risk",Possible,Moderate
,Accountability of agent actions,"When an agent acts autonomously, it becomes unclear who is responsible for outcomes. This creates governance complexity and can lead to regulatory problems.","Compliance risk, Legal risk, Operational risk",Possible,Major
,Compliance difficulties,"Autonomous actions may unintentionally violate rules, contracts or regulatory requirements. Ensuring continuous compliance becomes harder as agents evolve.","Compliance risk, Legal risk, Operational risk",Possible,Major
,Training data risks,,,,
,,,,,
,Risk name,Description,Categories,Likelihood,Severity
,Unrepresentative training distribution,The dataset doesn’t accurately reflect real-world scenarios or user populations. This causes uneven model performance and unreliable predictions in critical contexts.,"Operational risk, Technological risk, Compliance risk",Possible,Major
,Training data contamination,"The dataset contains irrelevant, corrupted or low-quality examples, which distort model learning. These impurities often lead to unstable or inconsistent model behavior.","Technological risk, Operational risk",Possible,Moderate
,Model overfitting during training,The model memorizes specific training examples instead of learning generalizable patterns. This results in degraded performance when encountering new or unseen data.,"Technological risk, Operational risk",Possible,Moderate
,Embedded dataset bias,"Harmful societal or historical biases are present in the training data and become encoded in the model. This leads to unfair outcomes, discrimination and compliance issues.","Compliance risk, Legal risk, Reputational risk, Human resources risk",Likely,Major
,Poor data curation practices,"Data is collected, filtered or prepared inconsistently, without quality controls or ethical oversight. These weaknesses undermine the reliability and fairness of the model.","Operational risk, Compliance risk",Possible,Moderate
,Risky or incorrect retraining,"When retraining processes are unmanaged, new data can introduce regressions or erase important learned behaviors. Models may degrade without clear warning.","Operational risk, Technological risk",Possible,Major
,Malicious data poisoning,Attackers or compromised pipelines may inject deceptive samples into the training dataset. This allows models to be manipulated into harmful or targeted behaviors.,"Cybersecurity risk, Operational risk",Unlikely,Major
,Presence of personal data in training,The dataset unintentionally includes personal or sensitive information. This creates regulatory exposure and ethical concerns if the model reproduces this content.,"Data privacy risk, Legal risk, Compliance risk",Possible,Major
,Reidentification through dataset content,Even anonymized training data can sometimes be matched with external datasets to reveal identities. This undermines privacy protections and exposes organizations to scrutiny.,"Data privacy risk, Compliance risk, Legal risk",Unlikely,Major
,Misalignment with data privacy rights,"The collected data may not comply with user consent, regional data laws or deletion requests. These gaps can trigger legal disputes or force retraining from scratch.","Data privacy risk, Legal risk, Compliance risk",Possible,Major
,Opaque training dataset,"Teams lack visibility into how the dataset was sourced, filtered or labeled. This opacity makes audits difficult and weakens trust in the model’s foundations.","Compliance risk, Operational risk",Possible,Moderate
,Unverified dataset provenance,"The origin, licensing or intended use of the data is unclear. This creates IP uncertainty and risks models being built on unauthorized or unethical sources.","Legal risk, Compliance risk, Third-party or vendor risk",Possible,Major
,Restrictions on data acquisition,Regulations or contractual terms limit which datasets the organization can legally collect or purchase. Violating these rules exposes projects to penalties or forced shutdowns.,"Legal risk, Compliance risk",Possible,Major
,Restrictions on data usage,Some data is licensed only for research or non-commercial use. Training models beyond those boundaries creates legal liabilities and reputational issues.,"Legal risk, Compliance risk",Possible,Major
,Restrictions on data transfer,"Cross-border data flows may violate regional laws or internal policies. These constraints impact cloud architecture, storage, and model training pipelines.","Compliance risk, Legal risk, Strategic risk",Possible,Major
,Confidential material embedded in training data,"Internal documents, trade secrets or customer records may appear in datasets without authorization. Models trained on such data risk exposing sensitive information later.","Data privacy risk, Cybersecurity risk, Legal risk",Possible,Major
,Insufficient rights to use training data,The dataset may be protected by copyright or license terms that don't allow model training or redistribution. Using it improperly can lead to lawsuits and loss of trust.,"Legal risk, Compliance risk, Strategic risk",Possible,Major
,Inference risks,,,,
,,,,,
,Risk name,Description,Categories,Likelihood,Severity
,Low inference accuracy,The model may generate incorrect outputs when handling real-world data not seen during training. These mistakes can be subtle and hard to detect in complex workflows. In high-impact domains this becomes a reliability concern.,"Operational risk, Technological risk",Possible,Moderate
,Evasion-based inference attack,Attackers craft inputs that intentionally mislead the model into producing wrong or unsafe outputs. These attacks often bypass normal guardrails and exploit model blind spots.,"Cybersecurity risk, Technological risk",Unlikely,Major
,Model extraction attempt,"An attacker sends repeated, carefully designed prompts to reverse-engineer parts of the model. Over time this leaks intellectual property and reduces competitive advantage.","Cybersecurity risk, Legal risk, Strategic risk",Possible,Major
,Jailbreaking the model,Users attempt to bypass safety rules through adversarial prompting or instruction manipulation. Successful jailbreaks allow harmful output generation or restricted functionality.,"Cybersecurity risk, Compliance risk, Operational risk",Likely,Major
,IP exposure in prompts,"Users may include confidential designs, source code or proprietary knowledge directly in prompts. This information may be logged, cached or indirectly surfaced later.","Data privacy risk, Legal risk, Reputational risk",Possible,Major
,Sensitive data exposure in prompts,"Prompts may contain personal information or confidential business details. If mishandled, this data can leak into analytics logs or reappear in outputs.","Data privacy risk, Legal risk, Compliance risk",Possible,Major
,Prompt injection attack,"A malicious actor embeds hidden instructions in input text, documents or URLs. The model follows these hidden commands, overriding intended behavior.","Cybersecurity risk, Operational risk",Likely,Major
,Prompt leaking through responses,"The model may reveal parts of its own system prompt, chain-of-thought hints or prior interactions. These disclosures weaken security and reveal internal logic.","Cybersecurity risk, Operational risk",Possible,Moderate
,Prompt priming manipulation,Attackers manipulate earlier conversation context to influence how the model interprets later prompts. This leads to biased or harmful outcomes without clear visibility.,"Cybersecurity risk, Operational risk",Possible,Moderate
,Context overload attack,"The attacker floods the context window with long or noisy text, pushing out relevant safety instructions or system messages. This degrades reliability and control.","Cybersecurity risk, Technological risk",Possible,Major
,Direct instruction override,Simple but forceful instructions are crafted to convince the model to ignore its guardrails. These attacks exploit limitations in safety alignment.,"Cybersecurity risk, Operational risk",Possible,Major
,Encoded malicious inputs,"Attackers embed harmful instructions inside encoded or obfuscated sequences (base64, unicode tricks, special tokens). Models may decode and execute them unintentionally.","Cybersecurity risk, Operational risk, Technological risk",Possible,Major
,Indirect-objective manipulation,"Attackers instruct the model through indirect cues, such as asking it to “emulate” another agent or persona. These pathways often bypass standard safeguards.","Cybersecurity risk, Operational risk",Possible,Moderate
,Social-engineering prompt attack,Attackers craft emotionally manipulative or authoritative prompts to trick the model into harmful behavior. These prompts exploit trust and politeness patterns in the model.,"Cybersecurity risk, Human resources risk, Operational risk",Likely,Major
,Special-token exploits,Models may respond unpredictably to rare or reserved tokens that behave differently from normal text. Attackers exploit these quirks to destabilize the system.,"Technological risk, Cybersecurity risk",Unlikely,Moderate
,Attribute inference attempt,Attackers analyze outputs to infer hidden attributes about the training data or demographics. This can reveal sensitive patterns or violate privacy expectations.,"Data privacy risk, Compliance risk",Unlikely,Major
,Membership inference attempt,Attackers try to determine whether specific individuals or records were used during training. Success exposes privacy gaps and weakens legal defensibility.,"Data privacy risk, Legal risk, Compliance risk",Unlikely,Major
,Output risks,,,,
,,,,,
,Risk name,Description,Categories,Likelihood,Severity
,Biased decision outputs,"The model may generate outputs that consistently favor or disadvantage certain groups. These biased patterns appear in scoring, recommendations or evaluations and can be difficult to detect. Organizations risk compliance violations and reputational harm.","Compliance risk, Legal risk, Reputational risk, Human resources risk",Likely,Major
,Skewed content generation,"Outputs may reflect subtle biases from training data, producing unbalanced summaries, suggestions or narratives. These distortions influence real decisions and user perceptions.","Operational risk, Reputational risk",Possible,Moderate
,Harmful advisory output,"The model may propose unsafe actions, incorrect instructions or dangerous recommendations. Users might act on them without verifying accuracy. This risk grows in domains like healthcare or finance.","Operational risk, Legal risk, Safety risk",Possible,Major
,Unsafe code generation,"When generating code, the model may introduce security vulnerabilities or flawed logic. Developers may trust the output too readily, leading to exploitable systems.","Cybersecurity risk, Technological risk, Operational risk",Likely,Major
,Toxic or offensive output,"The model may generate language that is insulting, harmful or inappropriate. Even rare incidents damage trust and can have regulatory implications.","Reputational risk, Compliance risk",Possible,Major
,Incomplete or misleading advice,The model may provide partially correct answers that omit critical details. These subtle gaps can mislead users more than outright errors.,"Operational risk, Safety risk",Possible,Moderate
,Overtrust in generated outputs,"Users may rely heavily on AI-generated responses without validating them. This leads to poor decision-making, especially in high-stakes environments.","Operational risk, Strategic risk",Likely,Moderate
,Harmful use of generated output,"End users may deliberately weaponize the model’s output (e.g., for fraud, manipulation or cyberattacks). The model becomes an indirect enabler of harmful activity.","Cybersecurity risk, Legal risk",Possible,Major
,Disinformation generation,The model can produce highly realistic but false content that spreads quickly. This can influence public opinion or damage institutional credibility.,"Reputational risk, Legal risk, Societal risk",Possible,Major
,Nonconsensual content generation,"The model may be used to generate content involving individuals without their consent, including impersonation. This creates ethical and privacy violations.","Legal risk, Data privacy risk, Reputational risk",Possible,Major
,Propagation of toxic content,The model may repeat or amplify harmful language it has been exposed to. These outputs negatively affect communities and user wellbeing.,"Reputational risk, Compliance risk",Possible,Moderate
,Improper contextual usage,"Outputs may be applied outside their intended purpose, leading to misuse. Even harmless-looking responses cause harm when placed into the wrong business flow.","Operational risk, Compliance risk",Possible,Moderate
,Lack of user disclosure,"When the output does not clarify that it is AI-generated, users may interpret it as authoritative or human-written. This transparency gap misleads stakeholders.","Compliance risk, Reputational risk",Likely,Moderate
,Hallucinatory incorrect output,"The model may confidently generate content that is entirely fabricated. These hallucinations often appear plausible, increasing the risk of being taken as truth.","Operational risk, Safety risk, Reputational risk",Likely,Major
,Exposure of private information through output,The model might unintentionally output private data learned from training or previous interactions. This creates legal exposure and privacy violations.,"Data privacy risk, Legal risk, Compliance risk",Possible,Major
,Copyright-sensitive content generation,Outputs may resemble or replicate copyrighted material. This introduces intellectual property disputes and business risk.,"Legal risk, Compliance risk, Reputational risk",Possible,Major
,Disclosure of confidential information,"The model can unknowingly reproduce internal documents, secrets or proprietary designs. These exposures are difficult to detect and mitigate post-incident.","Cybersecurity risk, Data privacy risk, Legal risk",Possible,Major
,Opaque reasoning in output,The model produces conclusions without explaining how it arrived at them. This reduces reliability and complicates human validation.,"Operational risk, Compliance risk",Possible,Moderate
,Unreliable source attribution,"When asked for citations, the model may fabricate sources or misattribute information. This damages credibility and can mislead users who rely on accuracy.","Operational risk, Reputational risk",Possible,Moderate
,Untraceable output origins,"It becomes unclear whether an output was generated from training data, retrieved material or internal heuristics. This ambiguity obstructs audits and legal review.","Compliance risk, Legal risk",Possible,Moderate
,Inaccessible training-source context,"Users and auditors cannot determine the training datasets behind specific outputs. This makes it difficult to assess fairness, privacy exposure or legal compliance.","Compliance risk, Legal risk, Operational risk",Possible,Moderate
,Non technical risks,,,,
,,,,,
,Risk name,Description,Categories,Likelihood,Severity
,Insufficient data transparency,"Teams may not clearly understand how data was sourced, processed or validated before being used in an AI system. This lack of clarity complicates audits and weakens trust. It often leads to compliance gaps.","Compliance risk, Operational risk",Possible,Moderate
,Lack of model transparency,"The internal workings of the model remain unclear to developers, auditors or stakeholders. This makes it difficult to explain decisions or diagnose harmful outcomes. Transparency gaps often escalate oversight issues.","Compliance risk, Operational risk, Reputational risk",Possible,Major
,Opaque system-level behavior,"Even if individual components are documented, the full system behavior becomes unpredictable when interactions scale. Organizations struggle to map cause and effect across the pipeline.","Operational risk, Strategic risk",Possible,Moderate
,Insufficient domain expertise,"Teams building or deploying models may lack deep knowledge of the domain where the AI is applied. This leads to misinterpretation of outputs, weak oversight and flawed decision-making.","Human resources risk, Operational risk",Likely,Moderate
,Poor use-case definition,"AI systems may be deployed into workflows without clearly defining scope, boundaries or intended outcomes. Ambiguous goals increase the risk of misuse or misalignment.","Strategic risk, Operational risk",Possible,Moderate
,Unrepresentative risk testing,"Testing may focus on narrow scenarios and fail to simulate real-world conditions. Blind spots emerge, especially in edge cases or diverse population segments.","Compliance risk, Operational risk",Possible,Major
,Incorrect or shallow risk testing,"Tests may be rushed, incomplete or misaligned with actual deployment conditions. This produces a false sense of confidence and allows major issues to escape detection.","Operational risk, Compliance risk",Possible,Major
,Lack of diversity in test coverage,"Testing may omit demographic groups, environmental conditions or linguistic variations. AI performance then becomes uneven across users and contexts.","Compliance risk, Reputational risk",Possible,Major
,Temporal drift in performance,"AI systems degrade over time because real-world data shifts, user behavior changes or context evolves. Without continuous monitoring, the system becomes less accurate and less safe.","Operational risk, Strategic risk",Likely,Moderate
,Model usage rights uncertainty,Teams may be unsure whether they legally have the rights to use a model for certain purposes. These uncertainties create legal exposure and can halt deployments abruptly.,"Legal risk, Compliance risk",Possible,Major
,Ambiguous legal accountability,"It may be unclear who is legally responsible for an AI-driven decision or outcome. This creates conflicts between teams, vendors and regulators, especially during incidents.","Legal risk, Compliance risk, Strategic risk",Possible,Major
,Unclear ownership of generated content,AI-generated outputs may not have a clear owner under current IP frameworks. This ambiguity complicates commercial use and contract obligations.,"Legal risk, Strategic risk",Possible,Moderate
,Environmental impact of AI operations,"Large-scale model training and inference consume significant energy. Without monitoring, organizations may exceed sustainability targets or face public criticism.","Environmental risk, Reputational risk",Possible,Minor
,Negative impact on affected communities,"AI systems may reinforce stereotypes, misrepresent groups or cause real-world harm in marginalized communities. These harms often surface slowly and are difficult to reverse.","Reputational risk, Compliance risk, Societal risk",Possible,Major
,Human exploitation through automation,"AI-driven workflows may pressure workers, accelerate productivity expectations or automate oversight unfairly. These issues create workplace inequality and ethical concerns.","Human resources risk, Reputational risk",Possible,Moderate
,Workforce disruption and job displacement,AI adoption can restructure teams and eliminate roles unexpectedly. Poor communication and planning amplify morale loss and resistance to adoption.,"Human resources risk, Strategic risk",Likely,Moderate
,Reduced human agency in oversight processes,"As AI automates decision-making, human reviewers may be sidelined or feel unable to intervene. This weakens governance and accountability mechanisms.","Strategic risk, Compliance risk, Operational risk",Possible,Major
,Cultural homogenization via AI systems,"AI-driven outputs may standardize communication, creativity and expression in ways that suppress cultural diversity. This subtly shapes user behavior over time.","Societal risk, Reputational risk",Unlikely,Moderate
,Academic integrity risks,"Students or researchers may misuse AI to bypass learning, write assignments or generate research. This erodes educational trust and assessment integrity.","Educational risk, Reputational risk",Likely,Moderate
,AI-enabled plagiarism,"AI can generate content that is too similar to copyrighted or academic sources. Users may unknowingly commit plagiarism, triggering disciplinary or legal consequences.","Educational risk, Legal risk, Reputational risk",Likely,Major
,Exclusion of certain user groups,"AI systems may work poorly for users with specific accents, disabilities, languages or cultural backgrounds. This exclusion reduces accessibility and harms user experience.","Compliance risk, Societal risk, Reputational risk",Possible,Major
,GenAI amplified risks,,,,
,,,,,
,Risk name,Description,Categories,Likelihood,Severity
,Realistic but incorrect content generation,Generative models can produce text that looks polished and authoritative but is factually wrong. These errors are more likely to be believed because of the natural writing style. This increases misinformation risks and misinformed decision-making.,"Operational risk, Reputational risk, Safety risk",Likely,Major
,Highly plausible hallucinations,"Hallucinations in generative models often appear coherent and detailed, making them harder to detect. Users may adopt them as truth, causing cascading errors in workflows. This is especially dangerous in compliance-heavy domains.","Operational risk, Compliance risk, Reputational risk",Likely,Major
,Synthetic content abuse at scale,"Generative AI enables rapid creation of harmful media such as phishing emails, impersonations or fraud campaigns. Attackers gain efficiency and sophistication with minimal effort.","Cybersecurity risk, Societal risk, Legal risk",Likely,Major
,Manipulative or persuasive output,"Generative models can craft emotionally engaging or persuasive content that influences behavior. This raises risks of manipulation in politics, finance or consumer decisions.","Societal risk, Reputational risk, Legal risk",Possible,Major
,Deepfake-style synthetic media generation,"Models capable of generating images, video or audio can be misused to create realistic fabrications. These outputs can damage reputations, enable fraud or undermine trust.","Cybersecurity risk, Legal risk, Reputational risk",Possible,Major
,Unintentional disclosure of sensitive training patterns,"Generative models may reproduce phrases, structures or snippets that resemble sensitive training data. This happens even without explicit memorization and creates privacy/legal risk.","Data privacy risk, Legal risk, Compliance risk",Possible,Major
,Amplified copyright exposure,"Generated text or media may resemble copyrighted works, even when not an exact match. Organizations face IP risks when outputs are used commercially.","Legal risk, Compliance risk",Possible,Moderate
,Propagating biased generative patterns,"Generative models may amplify biases through storytelling, examples or structural patterns. These subtle biases spread through downstream content or decisions.","Compliance risk, Reputational risk, Societal risk",Likely,Major
,Unsafe reasoning chains,Generative models sometimes produce step-by-step reasoning that seems rational but contains flaws or unsafe assumptions. These errors mislead users and weaken safety.,"Safety risk, Operational risk",Possible,Major
,Self-reinforcing generative drift,"When generative outputs are fed back into systems as new data, the model begins amplifying its own artifacts and distortions. This causes long-term degradation of quality and fairness.","Technological risk, Operational risk, Strategic risk",Possible,Moderate
,Generative scaling misuse,"The ability to mass-produce text, audio or visuals at near-zero cost enables large-scale campaigns — both beneficial and harmful. Without controls, this increases societal and regulatory concerns.","Societal risk, Legal risk, Reputational risk",Likely,Major
,Contextual overfitting in generation,"Generative models sometimes overfit to the local context, producing outputs that reflect prompt bias rather than objective reasoning. This leads to skewed or misleading content.","Operational risk, Technological risk",Possible,Moderate
,Overpersonalized or invasive generation,"Models can produce content that feels too personalized based on small user inputs, making users feel surveilled or profiled. This harms trust and creates privacy tension.","Data privacy risk, Reputational risk",Possible,Moderate
,Loss of human creativity diversity,"Overuse of generative tools can lead to homogenized writing style, visuals or expression. Organizations may slowly lose creative originality in marketing, design or communication.","Strategic risk, Reputational risk, Societal risk",Unlikely,Moderate
,Misleading confidence in AI authority,"Generative AI tends to sound polished and confident, even when wrong. This stylistic authority encourages users to overtrust the system beyond safe limits.","Operational risk, Strategic risk",Likely,Moderate