# ‚úÖ Complete LLM Evaluation Pipeline - READY TO USE!

## üéâ What's Been Built

### Full Pipeline: Frontend ‚Üí Backend ‚Üí EvaluationModule

```
User clicks "Start Evaluation"
    ‚Üì
Frontend sends POST /api/deepeval/experiments
    ‚Üì
Backend creates experiment in database
    ‚Üì
Background task triggers evaluation runner
    ‚Üì
EvaluationModule runs inference & scoring
    ‚Üì
Results stored in database
    ‚Üì
Monitor dashboard shows metrics
    ‚Üì
Experiments tab shows results
```

---

## ‚úÖ What Works Right Now

### 1. **Complete UI Workflow** ‚úÖ

- ‚úÖ Projects list and creation
- ‚úÖ 4-step evaluation wizard (Model ‚Üí Dataset ‚Üí Judge LLM ‚Üí Metrics)
- ‚úÖ Submit creates experiment and runs evaluation in background
- ‚úÖ Monitor dashboard with real-time metrics
- ‚úÖ Experiments list (will show your runs)

### 2. **Backend API** ‚úÖ

- ‚úÖ Database tables created (evaluation_logs, evaluation_metrics, experiments)
- ‚úÖ 15+ API endpoints for logs, metrics, experiments
- ‚úÖ Multi-tenant support
- ‚úÖ Background task execution

### 3. **Evaluation Engine** ‚úÖ

- ‚úÖ ModelRunner supports: Ollama, OpenAI, HuggingFace, Anthropic, Gemini, etc.
- ‚úÖ DeepEval metrics: Answer Relevancy, Bias, Toxicity, Hallucination, Faithfulness
- ‚úÖ Automatic logging of all interactions
- ‚úÖ Results storage

---

## üß™ How to Test End-to-End

### Prerequisites

**1. Ollama Running:**

```bash
# Check if Ollama is running
ollama list

# If you see models, you're good!
# If not, start Ollama or pull a model:
ollama pull assistant
```

**2. Backend Running:**

```bash
cd /Users/efeacar/verifywise/EvalServer/src
source venv/bin/activate
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

**3. Frontend Running:**

```bash
cd /Users/efeacar/verifywise/Clients
npm run dev
```

### Test Evaluation Flow

#### Step 1: Create Evaluation

1. Go to `http://localhost:5173/evals`
2. Click on your project (or create one)
3. Click **"New Eval"** button
4. Fill in the wizard:

   **Model Step:**

   - Select "Ollama"
   - Model Name: `assistant:latest` (or `gpt-oss:20b`)
   - Click Next

   **Dataset Step:**

   - Default dataset already loaded (11 prompts)
   - Click Next

   **Judge LLM Step:**

   - Select "Ollama"
   - Model Name: `assistant:latest`
   - Temperature: `0.7`
   - Max Tokens: `500`
   - Click Next

   **Metrics Step:**

   - Keep defaults (Answer Relevancy, Bias, Toxicity)
   - Click **"Start Evaluation"**

5. **What happens:**
   - ‚úÖ Alert: "Experiment created successfully!"
   - ‚úÖ Network tab shows: `POST /api/deepeval/experiments`
   - ‚úÖ Modal closes
   - ‚úÖ Backend starts running evaluation in background

#### Step 2: Watch Progress

**Option A: Backend Logs**
Watch your terminal where uvicorn is running - you'll see:

```
üì¶ Initializing model: assistant:latest
üìä Loading dataset: 11 prompts
ü§ñ Generating 11 responses...
  [1/11] Processing: Write a Python function...
     ‚úì Generated (1250ms)
  [2/11] Processing: Explain binary search...
     ‚úì Generated (1100ms)
...
üîç Running evaluation metrics...
‚úÖ Evaluation completed successfully!
```

**Option B: Monitor Tab**

1. Click **"Monitor"** tab
2. Click refresh button (or wait 30 seconds)
3. You should see:
   - Total Logs: 11 (one per prompt)
   - Avg Latency: ~1200ms
   - Recent Experiments: Your experiment with status "running" ‚Üí "completed"

#### Step 3: View Results

**Experiments Tab:**

1. Click "Experiments" tab
2. Your experiment will show:
   - Status: "completed" ‚úÖ
   - Results with scores
   - Timestamp

**Monitor Tab:**

1. Real-time metrics update
2. Performance graphs
3. Quality scores

---

## üìä What Gets Stored

### Experiment Record

```json
{
  "id": "exp_20250105_120000_123456",
  "project_id": "project_...",
  "name": "assistant:latest - 1/5/2025",
  "status": "completed",
  "config": {
    "model": {...},
    "judgeLlm": {...},
    "dataset": {11 prompts},
    "metrics": {...}
  },
  "results": {
    "total_prompts": 11,
    "avg_scores": {
      "answer_relevancy": 0.85,
      "bias": 0.05,
      "toxicity": 0.02
    }
  }
}
```

### Logs (11 entries - one per prompt)

```json
{
  "id": "uuid...",
  "experiment_id": "exp_...",
  "input": "Write a Python function to calculate factorial...",
  "output": "def factorial(n): ...",
  "model_name": "assistant:latest",
  "latency_ms": 1200,
  "token_count": 150,
  "status": "success"
}
```

### Metrics (Performance tracking)

```json
{
  "metric_name": "latency",
  "metric_type": "performance",
  "value": 1200.0,
  "experiment_id": "exp_..."
}
```

---

## üîß Configuration Notes

### API Keys

**For Judge LLM (Metrics):**

- **OpenAI**: Required for most DeepEval metrics
- **Other providers**: Ollama and HuggingFace don't need keys

**For Model Under Test:**

- Can be anything (Ollama recommended for local testing)

**Setting API Keys:**

Option 1: In the wizard (secure - not stored)

```
Judge LLM Step ‚Üí API Key field ‚Üí Enter your key
```

Option 2: Environment variable (for local dev)

```bash
export OPENAI_API_KEY="sk-..."
```

### Model Compatibility

**‚úÖ Tested & Working:**

- Ollama (local models)
- OpenAI (gpt-4, gpt-3.5-turbo)
- HuggingFace models

**‚ö†Ô∏è Requires Setup:**

- Anthropic: Need API key
- Gemini: Need API key
- xAI/Mistral: Need API key

---

## üìà Performance Expectations

**For 11 prompts (default dataset):**

- Time: ~2-5 minutes (depends on model speed)
- Logs: 11 entries
- Metrics: 4+ metric entries
- Cost: $0.00 (if using Ollama) or ~$0.05-0.10 (if using GPT-4)

**Status Progression:**

```
pending ‚Üí running (1-5 min) ‚Üí completed
```

---

## üêõ Troubleshooting

### "Model not found" Error

**Problem:** Ollama model doesn't exist locally

**Fix:**

```bash
# List available models
ollama list

# Pull a model if needed
ollama pull tinyllama
ollama pull llama2
```

### "No experiments shown"

**Check:**

1. Backend is running on port 8000
2. Database migrations ran successfully
3. Check Network tab for API errors

**Fix:**

```bash
# Verify tables exist
psql -U your_user -d your_database -c "
  SELECT table_name FROM information_schema.tables
  WHERE table_schema = 'a4ayc80OGd'
  AND table_name IN ('experiments', 'evaluation_logs');
"
```

### Monitor Shows "500 Internal Server Error"

**Already Fixed!** Refresh your browser.

If still happening:

```bash
# Re-run migrations
cd EvalServer/src
alembic upgrade head
```

---

## üéØ Quick Test Checklist

- [ ] Backend running on :8000
- [ ] Frontend running on :5173
- [ ] Ollama has models (`ollama list`)
- [ ] Can create projects
- [ ] "New Eval" button works
- [ ] Wizard completes all 4 steps
- [ ] "Start Evaluation" shows success alert
- [ ] Network tab shows POST /api/deepeval/experiments
- [ ] Experiments tab shows new experiment
- [ ] Monitor tab loads without errors
- [ ] Backend logs show evaluation progress

---

## üöÄ What's Next

### Immediate Enhancements

- [ ] Add progress indicator during evaluation
- [ ] Show real-time logs in UI
- [ ] Add "Stop" button for running evaluations
- [ ] Export results to CSV/JSON

### Phase 3 (from plan)

- [ ] Experiment comparison view
- [ ] More metrics (15+ additional scorers)
- [ ] Dataset versioning
- [ ] Playground for quick testing

---

## üìÅ Key Files

**Backend:**

- `/EvalServer/src/routers/evaluation_logs.py` - API routes
- `/EvalServer/src/utils/run_evaluation.py` - Evaluation runner
- `/EvalServer/src/crud/evaluation_logs.py` - Database operations

**Frontend:**

- `/Clients/src/presentation/pages/EvalsDashboard/NewExperimentModal.tsx` - Wizard
- `/Clients/src/presentation/pages/EvalsDashboard/ProjectMonitor.tsx` - Dashboard
- `/Clients/src/infrastructure/api/evaluationLogsService.ts` - API client

**Evaluation Engine:**

- `/EvaluationModule/src/deepeval_engine/model_runner.py` - Model inference
- `/EvaluationModule/src/deepeval_engine/deepeval_evaluator.py` - Metrics scoring

---

**Status:** ‚úÖ **FULLY FUNCTIONAL** - Ready for end-to-end testing!  
**Date:** January 5, 2025
