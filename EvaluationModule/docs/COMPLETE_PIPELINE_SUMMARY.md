# âœ… Complete LLM Evaluation Pipeline - READY TO USE!

## ğŸ‰ What's Been Built

### Full Pipeline: Frontend â†’ Backend â†’ EvaluationModule

```
User clicks "Start Evaluation"
    â†“
Frontend sends POST /api/deepeval/experiments
    â†“
Backend creates experiment in database
    â†“
Background task triggers evaluation runner
    â†“
EvaluationModule runs inference & scoring
    â†“
Results stored in database
    â†“
Monitor dashboard shows metrics
    â†“
Experiments tab shows results
```

---

## âœ… What Works Right Now

### 1. **Complete UI Workflow** âœ…

- âœ… Projects list and creation
- âœ… 4-step evaluation wizard (Model â†’ Dataset â†’ Judge LLM â†’ Metrics)
- âœ… Submit creates experiment and runs evaluation in background
- âœ… Monitor dashboard with real-time metrics
- âœ… Experiments list (will show your runs)

### 2. **Backend API** âœ…

- âœ… Database tables created (evaluation_logs, evaluation_metrics, experiments)
- âœ… 15+ API endpoints for logs, metrics, experiments
- âœ… Multi-tenant support
- âœ… Background task execution

### 3. **Evaluation Engine** âœ…

- âœ… ModelRunner supports: Ollama, OpenAI, HuggingFace, Anthropic, Gemini, etc.
- âœ… DeepEval metrics: Answer Relevancy, Bias, Toxicity, Hallucination, Faithfulness
- âœ… Automatic logging of all interactions
- âœ… Results storage

---

## ğŸ§ª How to Test End-to-End

### Prerequisites

**1. Ollama Running:**

```bash
# Check if Ollama is running
ollama list

# If you see models, you're good!
# If not, start Ollama or pull a model:
ollama pull assistant
```

**2. Backend Running:**

```bash
cd /Users/efeacar/verifywise/BiasAndFairnessServers/src
source venv/bin/activate
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

**3. Frontend Running:**

```bash
cd /Users/efeacar/verifywise/Clients
npm run dev
```

### Test Evaluation Flow

#### Step 1: Create Evaluation

1. Go to `http://localhost:5173/evals`
2. Click on your project (or create one)
3. Click **"New Eval"** button
4. Fill in the wizard:

   **Model Step:**

   - Select "Ollama"
   - Model Name: `assistant:latest` (or `gpt-oss:20b`)
   - Click Next

   **Dataset Step:**

   - Default dataset already loaded (11 prompts)
   - Click Next

   **Judge LLM Step:**

   - Select "Ollama"
   - Model Name: `assistant:latest`
   - Temperature: `0.7`
   - Max Tokens: `500`
   - Click Next

   **Metrics Step:**

   - Keep defaults (Answer Relevancy, Bias, Toxicity)
   - Click **"Start Evaluation"**

5. **What happens:**
   - âœ… Alert: "Experiment created successfully!"
   - âœ… Network tab shows: `POST /api/deepeval/experiments`
   - âœ… Modal closes
   - âœ… Backend starts running evaluation in background

#### Step 2: Watch Progress

**Option A: Backend Logs**
Watch your terminal where uvicorn is running - you'll see:

```
ğŸ“¦ Initializing model: assistant:latest
ğŸ“Š Loading dataset: 11 prompts
ğŸ¤– Generating 11 responses...
  [1/11] Processing: Write a Python function...
     âœ“ Generated (1250ms)
  [2/11] Processing: Explain binary search...
     âœ“ Generated (1100ms)
...
ğŸ” Running evaluation metrics...
âœ… Evaluation completed successfully!
```

**Option B: Monitor Tab**

1. Click **"Monitor"** tab
2. Click refresh button (or wait 30 seconds)
3. You should see:
   - Total Logs: 11 (one per prompt)
   - Avg Latency: ~1200ms
   - Recent Experiments: Your experiment with status "running" â†’ "completed"

#### Step 3: View Results

**Experiments Tab:**

1. Click "Experiments" tab
2. Your experiment will show:
   - Status: "completed" âœ…
   - Results with scores
   - Timestamp

**Monitor Tab:**

1. Real-time metrics update
2. Performance graphs
3. Quality scores

---

## ğŸ“Š What Gets Stored

### Experiment Record

```json
{
  "id": "exp_20250105_120000_123456",
  "project_id": "project_...",
  "name": "assistant:latest - 1/5/2025",
  "status": "completed",
  "config": {
    "model": {...},
    "judgeLlm": {...},
    "dataset": {11 prompts},
    "metrics": {...}
  },
  "results": {
    "total_prompts": 11,
    "avg_scores": {
      "answer_relevancy": 0.85,
      "bias": 0.05,
      "toxicity": 0.02
    }
  }
}
```

### Logs (11 entries - one per prompt)

```json
{
  "id": "uuid...",
  "experiment_id": "exp_...",
  "input": "Write a Python function to calculate factorial...",
  "output": "def factorial(n): ...",
  "model_name": "assistant:latest",
  "latency_ms": 1200,
  "token_count": 150,
  "status": "success"
}
```

### Metrics (Performance tracking)

```json
{
  "metric_name": "latency",
  "metric_type": "performance",
  "value": 1200.0,
  "experiment_id": "exp_..."
}
```

---

## ğŸ”§ Configuration Notes

### API Keys

**For Judge LLM (Metrics):**

- **OpenAI**: Required for most DeepEval metrics
- **Other providers**: Ollama and HuggingFace don't need keys

**For Model Under Test:**

- Can be anything (Ollama recommended for local testing)

**Setting API Keys:**

Option 1: In the wizard (secure - not stored)

```
Judge LLM Step â†’ API Key field â†’ Enter your key
```

Option 2: Environment variable (for local dev)

```bash
export OPENAI_API_KEY="sk-..."
```

### Model Compatibility

**âœ… Tested & Working:**

- Ollama (local models)
- OpenAI (gpt-4, gpt-3.5-turbo)
- HuggingFace models

**âš ï¸ Requires Setup:**

- Anthropic: Need API key
- Gemini: Need API key
- xAI/Mistral: Need API key

---

## ğŸ“ˆ Performance Expectations

**For 11 prompts (default dataset):**

- Time: ~2-5 minutes (depends on model speed)
- Logs: 11 entries
- Metrics: 4+ metric entries
- Cost: $0.00 (if using Ollama) or ~$0.05-0.10 (if using GPT-4)

**Status Progression:**

```
pending â†’ running (1-5 min) â†’ completed
```

---

## ğŸ› Troubleshooting

### "Model not found" Error

**Problem:** Ollama model doesn't exist locally

**Fix:**

```bash
# List available models
ollama list

# Pull a model if needed
ollama pull tinyllama
ollama pull llama2
```

### "No experiments shown"

**Check:**

1. Backend is running on port 8000
2. Database migrations ran successfully
3. Check Network tab for API errors

**Fix:**

```bash
# Verify tables exist
psql -U your_user -d your_database -c "
  SELECT table_name FROM information_schema.tables
  WHERE table_schema = 'a4ayc80OGd'
  AND table_name IN ('experiments', 'evaluation_logs');
"
```

### Monitor Shows "500 Internal Server Error"

**Already Fixed!** Refresh your browser.

If still happening:

```bash
# Re-run migrations
cd BiasAndFairnessServers/src
alembic upgrade head
```

---

## ğŸ¯ Quick Test Checklist

- [ ] Backend running on :8000
- [ ] Frontend running on :5173
- [ ] Ollama has models (`ollama list`)
- [ ] Can create projects
- [ ] "New Eval" button works
- [ ] Wizard completes all 4 steps
- [ ] "Start Evaluation" shows success alert
- [ ] Network tab shows POST /api/deepeval/experiments
- [ ] Experiments tab shows new experiment
- [ ] Monitor tab loads without errors
- [ ] Backend logs show evaluation progress

---

## ğŸš€ What's Next

### Immediate Enhancements

- [ ] Add progress indicator during evaluation
- [ ] Show real-time logs in UI
- [ ] Add "Stop" button for running evaluations
- [ ] Export results to CSV/JSON

### Phase 3 (from plan)

- [ ] Experiment comparison view
- [ ] More metrics (15+ additional scorers)
- [ ] Dataset versioning
- [ ] Playground for quick testing

---

## ğŸ“ Key Files

**Backend:**

- `/BiasAndFairnessServers/src/routers/evaluation_logs.py` - API routes
- `/BiasAndFairnessServers/src/utils/run_evaluation.py` - Evaluation runner
- `/BiasAndFairnessServers/src/crud/evaluation_logs.py` - Database operations

**Frontend:**

- `/Clients/src/presentation/pages/EvalsDashboard/NewExperimentModal.tsx` - Wizard
- `/Clients/src/presentation/pages/EvalsDashboard/ProjectMonitor.tsx` - Dashboard
- `/Clients/src/infrastructure/api/evaluationLogsService.ts` - API client

**Evaluation Engine:**

- `/EvaluationModule/src/deepeval_engine/model_runner.py` - Model inference
- `/EvaluationModule/src/deepeval_engine/deepeval_evaluator.py` - Metrics scoring

---

**Status:** âœ… **FULLY FUNCTIONAL** - Ready for end-to-end testing!  
**Date:** January 5, 2025
